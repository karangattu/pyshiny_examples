name: Inspect AI Evaluation

on:
  workflow_dispatch:
  pull_request:
    branches: [main]

# Cancel in-progress workflows for the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 30 # Prevent stuck workflows

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip" # Cache pip dependencies
          cache-dependency-path: |
            setup.py
            requirements*.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[test]"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('**/package-lock.json', '**/yarn.lock') }}

      - name: Install Playwright browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: playwright install

      # This new step replaces the three original steps with a loop
      - name: Run Evaluation and Tests 3 Times
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          set -e # Exit immediately if a command fails

          for i in {1..3}
          do
            echo "--- Starting Attempt $i of 3 ---"
            
            # Clean up results from previous attempt to ensure a clean slate
            rm -rf results/*
            rm -f test-results.xml

            echo "[Attempt $i] Creating test metadata..."
            python ./evals/create_test_metadata.py

            echo "[Attempt $i] Running Inspect AI evaluation..."
            inspect eval evals/evaluation.py@shiny_test_evaluation \
              --log-dir results/ \
              --log-format json

            echo "[Attempt $i] Running Tests..."
            test_exit_code=0
            # Disable exit on error just for the pytest command to check the exit code
            set +e
            pytest --tb=short --disable-warnings -n auto --maxfail=2 --junit-xml=test-results.xml || test_exit_code=$?
            # Re-enable exit on error immediately
            set -e

            # Check if tests failed and how many failures occurred
            if [ "${test_exit_code:-0}" -ne 0 ]; then
              failure_count=$(grep -o 'failures="[0-9]*"' test-results.xml | grep -o '[0-9]*' || echo "0")
              echo "Found $failure_count test failures on attempt $i"
              
              # Fail the workflow if more than 1 test failed
              if [ "$failure_count" -gt 1 ]; then
                echo "More than 1 test failed on attempt $i - failing CI"
                exit 1
              fi
            fi
            echo "--- Attempt $i of 3 Succeeded ---"
          done
          
          echo "All 3 evaluation and test runs passed successfully."

      - name: Process Results
        run: |
          python scripts/process_results.py results/*json

      - name: Check Quality Gate
        run: |
          python scripts/quality_gate.py results/

      - name: Comment PR Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results/summary.json', 'utf8'));

            const comment = `## Inspect AI Evaluation Results

            - **Tests Passed**: ${results.passed}/${results.total}
            - **Quality Gate**: ${results.quality_gate_passed ? '✅ PASSED' : '❌ FAILED'}

            ### Details
            ${results.details}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
