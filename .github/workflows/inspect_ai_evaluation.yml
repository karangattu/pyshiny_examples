name: Inspect AI Evaluation

on:
  workflow_dispatch:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  evaluate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -e ".[test]"

      - name: Install Playwright browsers
        run: |
          playwright install

      - name: Run Inspect AI evaluation
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python ./evals/create_test_metadata.py
          inspect eval evals/evaluation.py@shiny_test_evaluation --log-dir results/ --log-format json

          
      - name: Run Tests
        run: |
          # Run tests once with JUnit XML output to capture results
          pytest --tb=short --disable-warnings --maxfail=2 --junit-xml=test-results.xml || test_exit_code=$?
          
          # Count failures from XML output if tests failed
          if [ "${test_exit_code:-0}" -ne 0 ]; then
            failure_count=$(grep -o 'failures="[0-9]*"' test-results.xml | grep -o '[0-9]*' || echo "0")
            echo "Found $failure_count test failures"
            
            # Allow CI to pass if only 1 test fails, fail if more than 1
            if [ "$failure_count" -le 1 ]; then
              echo "Only 1 test failed - allowing CI to pass"
              exit 0
            else
              echo "More than 1 test failed - failing CI"
              exit 1
            fi
          fi

      - name: Process Results
        run: |
          python scripts/process_results.py results/*json

      - name: Check Quality Gate
        run: |
          python scripts/quality_gate.py results/

      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: inspect-ai-results
          path: results/

      - name: Comment PR Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results/summary.json', 'utf8'));
            
            const comment = `## Inspect AI Evaluation Results
            
            - **Tests Passed**: ${results.passed}/${results.total}
            - **Quality Gate**: ${results.quality_gate_passed ? '✅ PASSED' : '❌ FAILED'}
            
            ### Details
            ${results.details}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
