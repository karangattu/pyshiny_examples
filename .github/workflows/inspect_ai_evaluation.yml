name: Inspect AI Evaluation

on:
  workflow_dispatch:
  pull_request:
    branches: [main]

# Cancel in-progress workflows for the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 30 # Prevent stuck workflows

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip" # Cache pip dependencies
          cache-dependency-path: |
            setup.py
            requirements*.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[test]"

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('**/package-lock.json', '**/yarn.lock') }}

      - name: Install Playwright browsers
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: playwright install

      - name: Create test metadata
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: python ./evals/create_test_metadata.py

      - name: Run Inspect AI evaluation
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          inspect eval evals/evaluation.py@shiny_test_evaluation \
            --log-dir results/ \
            --log-format json

      - name: Run Tests
        id: tests
        run: |
          # Run tests with better error handling
          set +e  # Don't exit on error
          # Run tests once with JUnit XML output to capture results
          pytest --tb=short --disable-warnings -n auto --maxfail=2 --junit-xml=test-results.xml || test_exit_code=$?

          # Count failures from XML output if tests failed
          if [ "${test_exit_code:-0}" -ne 0 ]; then
            failure_count=$(grep -o 'failures="[0-9]*"' test-results.xml | grep -o '[0-9]*' || echo "0")
            echo "Found $failure_count test failures"
            
            # Allow CI to pass if only 1 test fails, fail if more than 1
            if [ "$failure_count" -le 1 ]; then
              echo "Only 1 test failed - allowing CI to pass"
              exit 0
            else
              echo "More than 1 test failed - failing CI"
              exit 1
            fi
          fi

      - name: Process Results
        run: |
          python scripts/process_results.py results/*json

      - name: Check Quality Gate
        run: |
          python scripts/quality_gate.py results/

      # - name: Upload Results
      #   uses: actions/upload-artifact@v4
      #   if: always()
      #   with:
      #     name: inspect-ai-results
      #     path: results/

      - name: Comment PR Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('results/summary.json', 'utf8'));

            const comment = `## Inspect AI Evaluation Results

            - **Tests Passed**: ${results.passed}/${results.total}
            - **Quality Gate**: ${results.quality_gate_passed ? '✅ PASSED' : '❌ FAILED'}

            ### Details
            ${results.details}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
